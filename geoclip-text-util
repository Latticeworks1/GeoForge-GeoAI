import torch
import torch.nn.functional as F
from PIL import Image
from geoclip import GeoCLIP, LocationEncoder
from transformers import CLIPTokenizer
import time
import os
import psutil
import numpy as np
from typing import Union, List, Tuple, Dict, Any, Optional
from dataclasses import dataclass
import platform

@dataclass
class InferenceMetrics:
    """Structured metrics for GeoCLIP inference"""
    inference_time: float  # seconds
    memory_used: float  # MB
    gpu_memory_used: Optional[float]  # MB
    cpu_percent: float  # %
    batch_size: int
    input_type: str  # "image" or "text"
    throughput: float  # queries/second

class GeoCLIPAPI:
    """Enhanced GeoCLIP API with performance metrics and resource tracking"""
    
    def __init__(self, device=None, precompute_gallery=True):
        """Initialize GeoCLIP model, tokenizer and resources"""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu") if device is None else device
        self.model = GeoCLIP().to(self.device)
        self.tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14")
        self.location_encoder = LocationEncoder().to(self.device)
        
        # System info
        self.has_gpu = torch.cuda.is_available()
        self.system_info = {
            "platform": platform.system(),
            "processor": platform.processor(),
            "python_version": platform.python_version(),
            "torch_version": torch.__version__,
            "gpu": torch.cuda.get_device_name(0) if self.has_gpu else "None"
        }
        
        # Precompute gallery for faster inference
        if precompute_gallery:
            start = time.time()
            with torch.no_grad():
                self.gallery = self.model.gps_gallery.to(self.device)
                self.gallery_features = F.normalize(self.model.location_encoder(self.gallery), dim=1)
            print(f"Precomputed gallery features in {time.time()-start:.3f}s")
    
    def _get_resource_usage(self) -> Dict[str, float]:
        """Get current resource usage"""
        metrics = {
            "memory_used": psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024),  # MB
            "cpu_percent": psutil.cpu_percent(),
        }
        
        if self.has_gpu:
            try:
                gpu_memory = torch.cuda.memory_allocated() / (1024 * 1024)  # MB
                metrics["gpu_memory_used"] = gpu_memory
            except Exception as e:
                metrics["gpu_memory_used"] = None
        else:
            metrics["gpu_memory_used"] = None
            
        return metrics
    
    @torch.no_grad()
    def predict(self, query: Union[str, List[str], Image.Image], top_k: int = 5, 
                return_metrics: bool = True) -> Dict[str, Any]:
        """
        Unified prediction method handling both images and text with performance metrics
        
        Args:
            query: Image path, PIL image, text query, or list of queries
            top_k: Number of predictions to return
            return_metrics: Whether to return performance metrics
            
        Returns:
            Dictionary with predictions and optional performance metrics
        """
        batch_size = 1
        input_type = None
        
        # Determine input type and prepare batch
        if isinstance(query, list):
            batch_size = len(query)
            if all(isinstance(q, str) for q in query):
                # Check if strings are file paths or text queries
                if all(os.path.exists(q) for q in query):
                    input_type = "image"
                else:
                    input_type = "text"
            elif all(isinstance(q, Image.Image) for q in query):
                input_type = "image"
        elif isinstance(query, str):
            if os.path.exists(query) and query.lower().endswith(('.png', '.jpg', '.jpeg', '.webp')):
                input_type = "image"
            else:
                input_type = "text"
        elif isinstance(query, Image.Image):
            input_type = "image"
        
        if input_type is None:
            raise ValueError("Query must be an image path, PIL image, text query, or list of either")
        
        # Capture starting resource metrics
        start_resources = self._get_resource_usage()
        start_time = time.time()
        
        # Perform prediction based on input type
        if input_type == "image":
            predictions = self._predict_from_image(query, top_k)
        else:
            predictions = self._predict_from_text(query, top_k)
        
        # Calculate metrics
        inference_time = time.time() - start_time
        end_resources = self._get_resource_usage()
        
        result = {"predictions": predictions}
        
        if return_metrics:
            metrics = InferenceMetrics(
                inference_time=inference_time,
                memory_used=end_resources["memory_used"] - start_resources["memory_used"],
                gpu_memory_used=end_resources.get("gpu_memory_used", None),
                cpu_percent=end_resources["cpu_percent"],
                batch_size=batch_size,
                input_type=input_type,
                throughput=batch_size / inference_time
            )
            result["metrics"] = metrics
            
        return result
    
    def _predict_from_image(self, query: Union[str, Image.Image, List[Union[str, Image.Image]]], 
                           top_k: int = 5) -> List[Dict[str, Any]]:
        """Internal method to predict from images"""
        if not isinstance(query, list):
            query = [query]
            
        results = []
        for q in query:
            try:
                # Handle image path or PIL image
                if isinstance(q, str):
                    img = Image.open(q).convert("RGB")
                    source = q
                else:
                    img = q
                    source = "PIL_Image"
                
                # Preprocess and get predictions
                img_tensor = self.model.image_encoder.preprocess_image(img).to(self.device)
                image_features = F.normalize(self.model.image_encoder(img_tensor), dim=1)
                
                # Calculate similarity with precomputed gallery
                similarity = self.model.logit_scale.exp() * (image_features @ self.gallery_features.T)
                probs = similarity.softmax(dim=-1)
                top_indices = torch.topk(probs[0], top_k)
                
                # Format results
                predictions = []
                for idx, prob in zip(top_indices.indices, top_indices.values):
                    coords = self.gallery[idx].cpu().numpy()
                    predictions.append({
                        "lat": float(coords[0]),
                        "lon": float(coords[1]),
                        "probability": float(prob)
                    })
                
                results.append({
                    "source": source,
                    "locations": predictions
                })
                
            except Exception as e:
                results.append({
                    "source": str(q) if isinstance(q, str) else "PIL_Image",
                    "error": str(e)
                })
        
        return results
    
    def _predict_from_text(self, query: Union[str, List[str]], top_k: int = 5) -> List[Dict[str, Any]]:
        """Internal method to predict from text"""
        if not isinstance(query, list):
            query = [query]
            
        results = []
        for q in query:
            try:
                # Process text query
                inputs = self.tokenizer(q, return_tensors="pt", padding=True, truncation=True).to(self.device)
                text_features = self.model.image_encoder.CLIP.get_text_features(**inputs)
                text_features = F.normalize(self.model.image_encoder.mlp(text_features), dim=1)
                
                # Calculate similarity with precomputed gallery
                similarity = self.model.logit_scale.exp() * (text_features @ self.gallery_features.T)
                probs = similarity.softmax(dim=-1)
                top_indices = torch.topk(probs[0], top_k)
                
                # Format results
                predictions = []
                for idx, prob in zip(top_indices.indices, top_indices.values):
                    coords = self.gallery[idx].cpu().numpy()
                    predictions.append({
                        "lat": float(coords[0]),
                        "lon": float(coords[1]),
                        "probability": float(prob)
                    })
                
                results.append({
                    "source": q,
                    "locations": predictions
                })
                
            except Exception as e:
                results.append({
                    "source": q,
                    "error": str(e)
                })
        
        return results
    
    def encode_gps(self, coordinates: Union[np.ndarray, torch.Tensor, List[List[float]]]) -> torch.Tensor:
        """
        Encode GPS coordinates into 512-dimensional embedding space
        
        Args:
            coordinates: Array of lat/lon pairs with shape (n, 2)
            
        Returns:
            Normalized embeddings with shape (n, 512)
        """
        start = time.time()
        if not isinstance(coordinates, torch.Tensor):
            coordinates = torch.tensor(coordinates, dtype=torch.float32)
        
        with torch.no_grad():
            embeddings = F.normalize(self.location_encoder(coordinates.to(self.device)), dim=1)
        
        print(f"Encoded {len(coordinates)} coordinates in {time.time()-start:.3f}s")
        return embeddings
    
    def compare_locations(self, coordinates: Union[np.ndarray, torch.Tensor, List[List[float]]]) -> np.ndarray:
        """
        Compute similarity matrix between locations
        
        Args:
            coordinates: Array of lat/lon pairs with shape (n, 2)
            
        Returns:
            n√ón similarity matrix
        """
        embeddings = self.encode_gps(coordinates)
        similarity = (embeddings @ embeddings.T).cpu().numpy()
        return similarity

def pretty_print_predictions(predictions: List[Dict[str, Any]], title: str):
    """
    Nicely format and print prediction results.
    """
    for result in predictions:
        source = result.get("source", "Unknown")
        locations = result.get("locations", [])
        print(f"{title}:")
        print(f"[*]- {source} - Top {len(locations)} (")
        for i, loc in enumerate(locations, start=1):
            lat, lon, prob = loc['lat'], loc['lon'], loc['probability']
            print(f"  {i}. ({lat:.6f}, {lon:.6f}) - {prob:.6f}")
        print(")\n")

def pretty_print_metrics(metrics: InferenceMetrics):
    """
    Nicely format and print performance metrics.
    """
    print("Performance Metrics:")
    print(f"  Inference time: {metrics.inference_time:.3f} s")
    print(f"  Memory used: {metrics.memory_used:.2f} MB")
    if metrics.gpu_memory_used is not None:
        print(f"  GPU memory used: {metrics.gpu_memory_used:.2f} MB")
    print(f"  CPU usage: {metrics.cpu_percent:.1f} %")
    print(f"  Throughput: {metrics.throughput:.2f} queries/s\n")

# End-to-end usage example
if __name__ == "__main__":
    # Initialize API
    api = GeoCLIPAPI()
    
    # Print system information
    print("System Information:")
    for key, value in api.system_info.items():
        print(f"  {key}: {value}")
    print("\n")
    
    # Improved image prediction output
    image_path = "/content/ellis.jpg"
    if os.path.exists(image_path):
        result = api.predict(image_path, return_metrics=True)
        pretty_print_predictions(result["predictions"], "Location")
        pretty_print_metrics(result["metrics"])
    
    # Improved text prediction output
    text_query = "mountain landscape with snow peaks"
    result = api.predict(text_query, return_metrics=True)
    pretty_print_predictions(result["predictions"], "Location")
    pretty_print_metrics(result["metrics"])
